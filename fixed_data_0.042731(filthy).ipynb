{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import xgboost as xgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "from functools import partial\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "import geopandas as gpd\n",
    "\n",
    "import datetime\n",
    "import pytz\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "from tqdm import tqdm\n",
    "\n",
    "from catboost import CatBoostRegressor\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, roc_auc_score\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "\n",
    "from zipfile import ZipFile\n",
    "from tzwhere import tzwhere\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmse(y_true, y_pred):\n",
    "    return sqrt(mean_squared_error(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_geo_dataframe(filename, path='data'):\n",
    "    \"\"\" load geo dataframe with point geometries from files \"\"\"\n",
    "    \n",
    "    df = None\n",
    "    for f in tqdm(os.listdir(path)):\n",
    "        if os.path.isdir(os.path.join(path,f)):\n",
    "            if df is None:\n",
    "                df = gpd.read_file(os.path.join(path, f, filename))\n",
    "            else:\n",
    "                df = df.append(gpd.read_file(os.path.join(path, f, filename)))\n",
    "\n",
    "    df['lat'] = df.geometry.y\n",
    "    df['long'] = df.geometry.x\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = 6373.0 # радиус земли в километрах\n",
    "\n",
    "def distance(x,y):\n",
    "    \"\"\"\n",
    "    Параметры\n",
    "    ----------\n",
    "    x : tuple, широта и долгота первой геокоординаты \n",
    "    y : tuple, широта и долгота второй геокоординаты \n",
    "    \n",
    "    Результат\n",
    "    ----------\n",
    "    result : дистанция в километрах между двумя геокоординатами\n",
    "    \"\"\"\n",
    "    lat_a, long_a, lat_b, long_b = map(radians, [*x,*y])    \n",
    "    dlon = long_b - long_a\n",
    "    dlat = lat_b - lat_a\n",
    "    a = sin(dlat/2)**2 + cos(lat_a) * cos(lat_b) * sin(dlon/2)**2\n",
    "    c = 2 * atan2(sqrt(a), sqrt(1 - a))\n",
    "    return R * c\n",
    "\n",
    "def nearest_distances(data, initial_data=None, metric=distance, n_neighbors=10, return_more=None):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    knc = KNeighborsClassifier(metric=metric)\n",
    "    dots = data[['lat','long']].dropna()\n",
    "    knc.fit(X=dots , y=np.ones(dots.shape[0]))\n",
    "    if initial_data is None:\n",
    "        distances, indexes = knc.kneighbors(X=dots, n_neighbors=n_neighbors)\n",
    "    else:\n",
    "        distances, indexes = knc.kneighbors(X=initial_data, n_neighbors=n_neighbors)\n",
    "        \n",
    "    print('Finded neares distances in {}'.format(time.time() - start_time))\n",
    "    if return_more:\n",
    "        more = data[[return_more]].values[indexes]\n",
    "        return distances, more\n",
    "    else:\n",
    "        return distances\n",
    "\n",
    "def timezone_from_coordinates(lat, long, tzw):\n",
    "\n",
    "    return tzw.tzNameAt(lat, long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:22<00:00,  1.43s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:06<00:00,  2.51it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:10<00:00,  1.55it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 16/16 [00:11<00:00,  1.45it/s]\n"
     ]
    }
   ],
   "source": [
    "pois = load_geo_dataframe('gis_osm_pois_free_1.shp')\n",
    "transport = load_geo_dataframe('gis_osm_transport_free_1.shp')\n",
    "traffic = load_geo_dataframe('gis_osm_traffic_free_1.shp')\n",
    "cities = load_geo_dataframe('gis_osm_places_free_1.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "osm_categories = ['pharmacy','supermarket','clothes','doityourself','hairdresser','post_office', 'car_wash',\n",
    "                  'beverages','florist','mobile_phone_shop','beauty_shop','dentist','school','hospital','doctors',\n",
    "                  'furniture_shop','butcher','bakery','artwork','kiosk','shoe_shop','car_dealership','jeweller',\n",
    "                 'chemist','greengrocer','toy_shop','gift_shop','bookshop','optician','sports_shop','vending_any',\n",
    "                 'travel_agent','newsagent','stationery','veterinary','department_store','hostel','vending_parking',\n",
    "                 'outdoor_shop','laundry','motel','bicycle_shop','university','college','hunting_stand','hotel',\n",
    "                 'restaurant','fast_food','bar','nightclub','food_court','bycicle_rental','cinema','theatre','arts_centre',\n",
    "                  'swimming_pool','museum','sports_centre','theme_park','zoo','stadium','mall']\n",
    "\n",
    "\n",
    "traffic_categories = ['fuel', 'parking', 'parking_underground', 'service']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "banks = pois[pois.fclass=='bank']\n",
    "atms = pois[pois.fclass=='atm']\n",
    "points = pois[pois.fclass.isin(osm_categories)]\n",
    "stops = transport[transport.fclass.isin(['bus_station','ferry_terminal'])]\n",
    "transport = transport[transport.fclass.isin(['railway_halt', 'railway_station'])]\n",
    "traffic = traffic[traffic.fclass.isin(traffic_categories)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv', index_col=0)\n",
    "test = pd.read_csv('test.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = pd.read_csv('fixed_yandex_coordinates.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# please, use YOUR Yandex API key\n",
    "# usage examples for geocoder https://tech.yandex.ru/maps/doc/geocoder/desc/examples/geocoder_examples-docpage/\n",
    "API_KEY = '6aadbc7c-f316-4d6d-8601-1d0fb1f69e22'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.address = [' '.join(adr.split()) for adr in train.address]\n",
    "test.address = [' '.join(adr.split()) for adr in test.address]\n",
    "\n",
    "coords.address = [' '.join(adr.split()) for adr in coords.address]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hot_fix = {'AVIAITSIONNAYA, 12 Komsomolsk-31': [50.263251, 137.476056, ',,Комсомольск-31'], \n",
    "          'N.TERNOVSKAYA, 1 Penza': [53.132400, 45.022825, ',,Пенза'], \n",
    "          'MSK KIEVSKOE 1 A MOSKVA': [55.625801, 37.439239, ',,Москва'], \n",
    "          'D. 2, UL. KRASNAYA GURYEVSK G': [54.770972, 20.606706, ',,Гурьевск'], \n",
    "          'MOZHAYSKOE SH.,80A Odintsovo': [55.660353, 37.188923, ',,Одинцово'], \n",
    "          'KOMSOM NA AMURE 37 Komsomolsk-31': [50.263251, 137.476056, ',,Комсомольск-31'], \n",
    "          'UL. KUTUZOVA VOLOGDA 20 N': [59.157670, 38.721192, ',,Вологда-20']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fix in hot_fix.keys():\n",
    "    coords.loc[coords.address==fix, 'lat'] = hot_fix[fix][0]\n",
    "    coords.loc[coords.address==fix, 'long'] = hot_fix[fix][1]\n",
    "    coords.loc[coords.address==fix, 'address_rus'] = hot_fix[fix][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train.loc[train.lat.notnull(), :].append(train.loc[train.lat.isnull(), :][[\n",
    "            'id', 'atm_group', 'address','target']].merge(coords, on='address', how='left'), sort=False).reset_index(drop=True)\n",
    "\n",
    "test_data = test.loc[test.lat.notnull(), :].append(test.loc[test.lat.isnull(), :][[\n",
    "            'id', 'atm_group', 'address']].merge(coords, on='address', how='left'), sort=False).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_data['target']\n",
    "train_data_copy = train_data.copy()\n",
    "train_data = train_data.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.append(test_data).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finded neares distances in 33.604456186294556\n",
      "Finded neares distances in 224.02339029312134\n",
      "Finded neares distances in 217.73690629005432\n",
      "Finded neares distances in 52.30057907104492\n",
      "Finded neares distances in 142.40742945671082\n",
      "Finded neares distances in 111.70325136184692\n",
      "Finded neares distances in 37.75513434410095\n",
      "Finded neares distances in 44.126532793045044\n"
     ]
    }
   ],
   "source": [
    "distances = nearest_distances(data=X, n_neighbors=6)\n",
    "\n",
    "distances_osm = nearest_distances(data=points, initial_data=X[['lat','long']].dropna(), n_neighbors=10)\n",
    "\n",
    "distances_cities = nearest_distances(data=cities, initial_data=X[['lat','long']].dropna(), n_neighbors=10)\n",
    "distances_transport = nearest_distances(data=transport, initial_data=X[['lat','long']].dropna(), n_neighbors=10)\n",
    "distances_traffic = nearest_distances(data=traffic, initial_data=X[['lat','long']].dropna(), n_neighbors=10)\n",
    "distances_bank = nearest_distances(data=banks, initial_data=X[['lat','long']].dropna(), n_neighbors=10)\n",
    "distances_atm = nearest_distances(data=atms, initial_data=X[['lat','long']].dropna(), n_neighbors=10)\n",
    "distances_stops = nearest_distances(data=stops, initial_data=X[['lat','long']].dropna(), n_neighbors=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finded neares distances in 2.815467596054077\n",
      "Finded neares distances in 18.242207527160645\n",
      "Finded neares distances in 6.844727516174316\n",
      "Finded neares distances in 17.898127555847168\n",
      "Finded neares distances in 11.44852089881897\n",
      "Finded neares distances in 28.485808849334717\n",
      "Finded neares distances in 14.868214845657349\n"
     ]
    }
   ],
   "source": [
    "distances_group = []\n",
    "for gr in X.atm_group.unique():\n",
    "    distances_group.append(nearest_distances(data=X[X.atm_group==gr], initial_data=X[['lat','long']].dropna(), n_neighbors=6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_features(df, distances, feature_prefix):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    for n in range(distances.shape[1]):\n",
    "        df_copy['{}_{}'.format(feature_prefix, n)] = distances[:, n]\n",
    "        \n",
    "    return df_copy\n",
    "\n",
    "def add_stat_distance_features(df, feature_prefix):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    df_copy['{}_mean'.format(feature_prefix)] = df_copy.iloc[:, df_copy.columns.str.contains(feature_prefix)].mean(axis=1)\n",
    "    # df_copy['{}_median'.format(feature_prefix)] = df_copy.iloc[:, df_copy.columns.str.contains(feature_prefix)].median(axis=1)\n",
    "    df_copy['{}_std'.format(feature_prefix)] = df_copy.iloc[:, df_copy.columns.str.contains(feature_prefix)].std(axis=1)\n",
    "    df_copy['{}_interquartile'.format(feature_prefix)] = df_copy.iloc[:, df_copy.columns.str.contains(feature_prefix)].quantile(0.75, axis=1) - \\\n",
    "                               df_copy.iloc[:, df_copy.columns.str.contains(feature_prefix)].quantile(0.25, axis=1)\n",
    "        \n",
    "    df_copy['{}_min_max'.format(feature_prefix)] = df_copy.iloc[:, df_copy.columns.str.contains(feature_prefix)].max(axis=1) - \\\n",
    "                         df_copy.iloc[:, df_copy.columns.str.contains(feature_prefix)].min(axis=1)\n",
    "        \n",
    "    return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_feat = add_distance_features(X[['lat','long']].dropna(), distances, 'distance_data')\n",
    "\n",
    "X_feat = add_distance_features(X_feat, distances_osm, 'distance_osm')\n",
    "\n",
    "X_feat = add_distance_features(X_feat, distances_transport, 'distance_transport')\n",
    "X_feat = add_distance_features(X_feat, distances_cities, 'distance_cities')\n",
    "X_feat = add_distance_features(X_feat, distances_traffic, 'distance_traffic')\n",
    "X_feat = add_distance_features(X_feat, distances_bank, 'distance_bank')\n",
    "X_feat = add_distance_features(X_feat, distances_atm, 'distance_atm')\n",
    "X_feat = add_distance_features(X_feat, distances_stops, 'distance_stops')\n",
    "\n",
    "for pref in ['distance_data','distance_osm','distance_transport',\n",
    "             'distance_cities','distance_traffic','distance_bank','distance_atm','distance_stops']:\n",
    "    X_feat = add_stat_distance_features(X_feat, pref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n, gr in enumerate(X.atm_group.unique()):\n",
    "    X_feat = add_distance_features(X_feat, distances_group[n], 'distance_group_{}'.format(gr))\n",
    "    X_feat = add_stat_distance_features(X_feat, 'distance_group_{}'.format(gr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.merge(X_feat.drop_duplicates(subset=['lat','long']), on=['lat','long'], sort=False, how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X['city'] = X[~X.address_rus.isnull()].address_rus.apply(lambda x: x.split(',')[2]) \n",
    "X['eng_city'] = [i.split(' ')[-1].lower() for i in X.address]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rare_cities = X.city.value_counts()[(X.city.value_counts() < 10) == True].index\n",
    "rare_eng_cities = X.eng_city.value_counts()[(X.eng_city.value_counts() < 10) == True].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.city = X.city.apply(lambda x: 'RARE' if x in rare_cities else x)\n",
    "X.eng_city = X.eng_city.apply(lambda x: 'RARE' if x in rare_eng_cities else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.city = X.city.rank().fillna(-1)\n",
    "X.eng_city = X.eng_city.rank().fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ = X[:len(train_data)]\n",
    "\n",
    "X_ = X_.drop(['address', 'address_rus','distance_data_0'], axis=1)\n",
    "\n",
    "Y_ = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Fold\n",
      "1 Fold\n",
      "2 Fold\n",
      "3 Fold\n",
      "4 Fold\n",
      "5 Fold\n",
      "6 Fold\n",
      "7 Fold\n",
      "8 Fold\n",
      "9 Fold\n",
      "results: [0.04193752242045955, 0.042053930097359, 0.041223266908563434, 0.04144114205972502, 0.041061398889181616, 0.04286462683382677, 0.04348723520218232, 0.0419338362252758, 0.04214691844699118, 0.04234620929009307], mean_result: 0.04205, std: 0.0007\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=10, random_state=1, shuffle=True)\n",
    "results = []\n",
    "models = []\n",
    "\n",
    "test_ys = []\n",
    "predictions = []\n",
    "\n",
    "n = 0\n",
    "\n",
    "\n",
    "for train_index, test_index in kf.split(X_):\n",
    "    \n",
    "    print(\"{} Fold\".format(n))\n",
    "    \n",
    "    X_train, X_valid = X_.loc[train_index, :], X_.loc[test_index, :]\n",
    "    Y_train, Y_valid = Y_[train_index], Y_[test_index]\n",
    "    \n",
    "    test_ys.append(np.stack([X_valid['id'].values, Y_valid.values], axis=0))\n",
    "\n",
    "    gbm = xgb.XGBRegressor(**{'alpha': 10.0, 'colsample_bytree': 1.0, 'eta': 0.2, 'gamma': 0.0, 'lambda': 0.9, \n",
    "                              'learning_rate': 0.03, 'max_depth': 11, 'min_child_weight': 10.0, 'n_estimators': 400, \n",
    "                              'subsample': 0.8, 'random_state': 1, 'n_jobs': -1})\n",
    "    \n",
    "    gbm1 = xgb.XGBRegressor(**{'alpha': 10.0, 'colsample_bytree': 1.0, 'eta': 0.2, 'gamma': 0.0, 'lambda': 0.9, \n",
    "                              'learning_rate': 0.03, 'max_depth': 11, 'min_child_weight': 10.0, 'n_estimators': 400, \n",
    "                              'subsample': 0.8, 'random_state': 4, 'n_jobs': -1})\n",
    "    \n",
    "    gbm2 = xgb.XGBRegressor(**{'alpha': 10.0, 'colsample_bytree': 1.0, 'eta': 0.2, 'gamma': 0.0, 'lambda': 0.9, \n",
    "                              'learning_rate': 0.03, 'max_depth': 11, 'min_child_weight': 10.0, 'n_estimators': 400, \n",
    "                              'subsample': 0.8, 'random_state': 9, 'n_jobs': -1})\n",
    "\n",
    "    gbm.fit(X_train, Y_train, eval_set=[(X_valid, Y_valid)], early_stopping_rounds=20, eval_metric='rmse',\n",
    "            verbose=False)\n",
    "    gbm1.fit(X_train, Y_train, eval_set=[(X_valid, Y_valid)], early_stopping_rounds=20, eval_metric='rmse',\n",
    "            verbose=False)\n",
    "    gbm2.fit(X_train, Y_train, eval_set=[(X_valid, Y_valid)], early_stopping_rounds=20, eval_metric='rmse',\n",
    "            verbose=False)\n",
    "    \n",
    "    \n",
    "    models.append(gbm)\n",
    "    predictions.append(gbm.predict(X_valid))\n",
    "    models.append(gbm1)\n",
    "    predictions.append(gbm1.predict(X_valid))\n",
    "    models.append(gbm2)\n",
    "    predictions.append(gbm2.predict(X_valid))\n",
    "\n",
    "    results.append(rmse(Y_valid, np.mean([gbm.predict(X_valid), gbm1.predict(X_valid), gbm2.predict(X_valid)], axis=0)))\n",
    "\n",
    "    n += 1\n",
    "    \n",
    "print(\"results: {}, mean_result: {:.5f}, std: {:.4f}\".format(results, np.mean(results), np.std(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots(figsize=(10,40))\n",
    "# xgb.plot_importance(gbm, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X[len(train_data):].drop(['address', 'address_rus', 'distance_data_0'], axis=1).sort_values(by='id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame(np.mean([models[n].predict(X_test) for n in range(len(models))], axis=0), \n",
    "                      index=test.sort_values(by='id').index, columns=['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.sort_index().to_csv('submit_not_fixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.00032762368, 0.0007147964950465309, 0.8223676156547468)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we have a little leak - mean of train target and predicted target is 0\n",
    "# and std of train targets and predicted targets must be 0.1 (because we divide by 10 our target)\n",
    "submit.target.mean(), np.mean(target), np.std(list(submit.target * 10) + list(target * 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let substract bias in train data\n",
    "submit.target = submit.target - (np.mean(target) + submit.target.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.sort_index().to_csv('submit_fixed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
